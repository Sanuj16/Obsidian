# 1.2.3 Choosing a Representation for the Target Funciton

We need as close a representation to the ideal target function $V$ as possible bur the more expressive it gets, the more traning data ut need to choose among the alernative hypothesis. $\hat{V}$ can be represented as:
- $x_1$ : the number of black pieces on the board
- $x_2$ : the number of red pieces on the board
- $x_3$ : the number of black kings on the board
- $x_4$ : the number of red kings on the board
- $x_5$ : the number of black pieces threatened by red (i.e., which can be captured
on red's next turn)
- $x_6$ : the number of red pieces threatened by black

Thus, our learning program will represent $\hat{V}(b)$ as a linear function of the form:
$\hat{V}(b) = \omega_{0}+ \omega_{1}x_{1} + \omega_{2}x_{2} +\omega_{3}x_{3} + \omega_{4}x_{4} + \omega_{5}x_{5} + \omega_{6}x_{6}$ 
Learned values for the weights $\omega_1$ through $\omega_6$ will determine the relative importance of the various board features in determining the value of the board, whereas the weight $\omega_0$ will provide an additive constant to the board value.

Partial design of a checkers learning program:
- Task $T$: playing checkers
- Performance measure $P$: percent of games won in the world tournament
- Training experience $E$: games played against itself
- Target function: $V:Board \to R$
- Target function representation:
	$\hat{V}(b) = \omega_{0}+ \omega_{1}x_{1} + \omega_{2}x_{2} +\omega_{3}x_{3} + \omega_{4}x_{4} + \omega_{5}x_{5} + \omega_{6}x_{6}$ 

---
# References


---
Date: 27-09-2022
Time: 04:16
Status: #note
Tags: #machinelearning